{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOBeGUf6aN+XAn2liPZk+AT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RDGopal/IB9AU-2026/blob/main/PyTorch_Tutorial_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Tutorial\n",
        "\n",
        "Welcome to the exciting intersection of Deep Learning and FinTech! In today's rapidly evolving financial landscape, traditional analytical methods are often stretched to their limits by the sheer volume, velocity, and variety of data. This is where **Deep Learning** steps in, offering powerful new capabilities to extract insights and make predictions from complex financial datasets.\n",
        "\n",
        "\n",
        "Imagine tackling challenges like:\n",
        "* *Fraud Detection:* Billions of transactions occur daily. Deep learning models can sift through these, identifying subtle, non-obvious patterns indicative of fraudulent activity with far greater accuracy and speed than rule-based systems.\n",
        "* *Algorithmic Trading:* Beyond simple strategies, deep learning can learn intricate relationships between market indicators, news sentiment, and price movements to inform sophisticated automated trading decisions.\n",
        "* *Risk Management:* Assessing credit risk, market risk, or operational risk becomes more nuanced when models can learn from diverse data sources, from individual financial histories to macroeconomic indicators, predicting potential defaults or market shocks more effectively.\n",
        "* *Time Series Forecasting:* Predicting stock prices, commodity futures, interest rates, or economic indicators are classic financial problems.\n",
        "\n",
        "These applications, and many more, highlight deep learning's potential to revolutionize how we understand and interact with financial markets.\n",
        "\n",
        "##Why PyTorch\n",
        "\n",
        "* **Flexibility and Control:** PyTorch is renowned for its highly flexible and intuitive design. It gives researchers and developers granular control over model architectures and training processes. This is particularly valuable in finance, where custom models and novel approaches are often needed to gain a competitive edge or address unique data characteristics. You're not just using a black box; you're building and understanding it from the ground up.\n",
        "* **Pythonic Nature:** If you're comfortable with Python, you'll feel right at home with PyTorch. Its API is designed to be very Python-friendly, making it easier to write, read, and debug code. This allows us to focus more on the financial problem at hand and less on wrestling with complex framework-specific syntax.\n",
        "* **Dynamic Computation Graph**: PyTorch uses a dynamic computation graph. This means the graph is built on-the-fly as operations are executed. What does this offer us?\n",
        "*  **Easier Debugging:** You can use standard Python debugging tools to step through your network's forward pass.\n",
        "*  **Greater Flexibility for Complex Models:** It allows for models with variable inputs, control flow statements (like if-else loops), and recursive neural networks, which are common in advanced research and can be beneficial for modeling the often-irregular nature of financial data or complex decision processes.\n",
        "\n",
        "In essence, PyTorch provides the robust tools, the intuitive interface, and the flexibility we need to effectively explore, build, and deploy deep learning solutions for the intricate challenges within the FinTech domain. Let's get started!\n",
        "\n",
        "Some resources:\n",
        "\n",
        " https://sebastianraschka.com/teaching/pytorch-1h/"
      ],
      "metadata": {
        "id": "x5YU-tegp63K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "As Pytorch is already pre-installed in Google Colab, you can just import it. The following shows the version of Pytorch and GPU availability."
      ],
      "metadata": {
        "id": "gdt8u0G-V_9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check PyTorch version and CUDA availability\n",
        "import torch\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "m7ag8SIoWAfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tensors: The Universal Language of Deep Learning\n",
        "At the heart of every PyTorch operation, every neural network layer, and every piece of data you'll encounter is a fundamental data structure called a Tensor.\n",
        "\n",
        "**What is a Tensor?**\n",
        "\n",
        "You can think of a Tensor as a highly optimized, multi-dimensional array, very much like a NumPy array, but with two crucial superpowers:\n",
        "1. GPU Acceleration: Tensors can seamlessly operate on Graphics Processing Units (GPUs). GPUs are specialized electronic circuits designed to rapidly manipulate and alter memory to accelerate the creation of images, but their parallel processing architecture makes them exceptionally good at the kind of large-scale matrix and vector computations that deep learning thrives on. When you run computations on GPU-enabled Tensors, you can see dramatic speedups, especially with large datasets and complex models.\n",
        "2. Automatic Differentiation (Autograd): This is the magic ingredient we'll explore shortly. PyTorch Tensors are designed to keep track of the operations performed on them, allowing for the automatic calculation of gradients—a cornerstone of how neural networks learn.\n",
        "\n",
        "In essence, Tensors are the fundamental data structures that PyTorch uses to encode the inputs and outputs of a model, as well as the model's parameters (like weights and biases).\n",
        "\n",
        "Scalars (0-D Tensor): A single number. E.g., `torch.tensor(5)`\n",
        "\n",
        "Vectors (1-D Tensor): A list of numbers. E.g., `torch.tensor([1, 2, 3])`\n",
        "\n",
        "Matrices (2-D Tensor): A grid of numbers. E.g., `torch.tensor([[1, 2], [3, 4]])`\n",
        "\n",
        "3-D Tensors and Beyond: For images (height, width, color channels) or sequences of data (batch size, sequence length, features), we use higher-dimensional tensors.\n",
        "\n"
      ],
      "metadata": {
        "id": "QKIltLzmFMtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Tensors:\n",
        "PyTorch provides intuitive ways to create tensors:"
      ],
      "metadata": {
        "id": "jQy6NwTWxkUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From Python Lists**: You can directly convert Python lists into tensors. This is often how you'll get started with small data."
      ],
      "metadata": {
        "id": "XY5WKPK-xw2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "my_list = [[1, 2, 3], [4, 5, 6]]\n",
        "tensor_from_list = torch.tensor(my_list)\n",
        "print(\"From List:\\n\", tensor_from_list)\n"
      ],
      "metadata": {
        "id": "gEzLEW7wx6jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From NumPy Arrays:** If you're working with data already in `NumPy`, converting to a PyTorch tensor is straightforward."
      ],
      "metadata": {
        "id": "p7h-H34RyBnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "numpy_array = np.array([[7, 8], [9, 10]])\n",
        "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
        "print(\"From NumPy:\\n\", tensor_from_numpy)"
      ],
      "metadata": {
        "id": "QxK3LUwdyQrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Tensors:** For initializing model weights or creating synthetic data, random tensors are invaluable."
      ],
      "metadata": {
        "id": "CdrTjinJyc-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_tensor = torch.rand(3, 4) # Creates a 3x4 tensor with random values between 0 and 1\n",
        "print(\"Random:\\n\", random_tensor)\n",
        "\n",
        "random_int_tensor = torch.randint(0, 10, (2, 2)) # Random integers between 0 (inclusive) and 10 (exclusive)\n",
        "print(\"Random Integers:\\n\", random_int_tensor)"
      ],
      "metadata": {
        "id": "ExK5ETKBy0h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensors of Zeros or Ones:** Useful for initialization or masks."
      ],
      "metadata": {
        "id": "vxIjQrmQzAQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zeros_tensor = torch.zeros(2, 3)\n",
        "print(\"Zeros:\\n\", zeros_tensor)\n",
        "\n",
        "ones_tensor = torch.ones(2, 2)\n",
        "print(\"Ones:\\n\", ones_tensor)"
      ],
      "metadata": {
        "id": "a790WHKxyiXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Tensor Operations: It's Like Math, But Faster!\n",
        "Once you have tensors, you can perform a wide range of mathematical operations on them, much like with `NumPy` arrays."
      ],
      "metadata": {
        "id": "QwYUWkVPzJAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Element-wise Addition, Subtraction, Multiplication, Division**: These operations apply to corresponding elements. Tensors must have compatible shapes."
      ],
      "metadata": {
        "id": "T8xswpNxzZsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.tensor([[1, 2], [3, 4]])\n",
        "t2 = torch.tensor([[5, 6], [7, 8]])\n",
        "print(\"t1:\\n\", t1)\n",
        "print(\"t2:\\n\", t2)\n",
        "\n",
        "sum_tensor = t1 + t2\n",
        "product_tensor = t1 * t2 # Element-wise multiplication\n",
        "print(\"Sum:\\n\", sum_tensor)\n",
        "print(\"Element-wise Product:\\n\", product_tensor)"
      ],
      "metadata": {
        "id": "Po2Gxn_jzedq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix Multiplication:** Crucial for neural networks. Use `@` for matrix multiplication or `torch.matmul()`."
      ],
      "metadata": {
        "id": "bOBZQqtoz8aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mat_product = t1 @ t2 # Matrix multiplication\n",
        "print(\"Matrix Product:\\n\", mat_product)"
      ],
      "metadata": {
        "id": "pvg5Hha1zSvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reshaping:** Changing the dimensions of a tensor without changing its data. This is fundamental for feeding data into different layers of a neural network."
      ],
      "metadata": {
        "id": "d2ozNhpx0RD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_tensor = torch.randn(2, 3) # A 2x3 tensor\n",
        "print(\"Original Shape:\", original_tensor.shape)\n",
        "print(\"Original Tensor\\n\", original_tensor)\n",
        "\n",
        "reshaped_tensor = original_tensor.view(3, 2) # Reshape to 3x2\n",
        "print(\"Reshaped (view) Shape:\\n\", reshaped_tensor.shape)\n",
        "print(\"Reshaped Tensor\\n\", reshaped_tensor)\n",
        "\n",
        "flattened_tensor = original_tensor.view(-1) # Flatten to 1D, -1 infers the size\n",
        "print(\"Flattened Shape:\\n\", flattened_tensor.shape)\n",
        "print(\"Flattened Tensor\\n\", flattened_tensor)\n",
        "# Note: .reshape() is another method, often preferred as it can handle non-contiguous memory\n",
        "# reshaped_tensor_2 = original_tensor.reshape(3, 2)"
      ],
      "metadata": {
        "id": "q-YONS7Z0V_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Device (CPU vs. GPU)**: Where the tensor's data is actually stored and computations are performed.\n",
        "* CPU (Central Processing Unit): The default device. All tensors are created on the CPU unless specified otherwise.\n",
        "* GPU (Graphics Processing Unit): If your Google Colab runtime is set to GPU (`Runtime -> Change runtime type -> Hardware accelerator -> GPU`) and `PyTorch` can detect it, you can move tensors to the GPU for faster computations.\n",
        "\n",
        "You move tensors (and eventually models) to a device using the `.to()` method:"
      ],
      "metadata": {
        "id": "ARsGAQ_q2bxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda') # Use GPU\n",
        "    print(\"CUDA (GPU) is available! Using GPU for tensors.\")\n",
        "else:\n",
        "    device = torch.device('cpu') # Fallback to CPU\n",
        "    print(\"CUDA (GPU) not available. Using CPU for tensors.\")\n",
        "\n",
        "# Create a tensor on the CPU\n",
        "cpu_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "print(f\"CPU Tensor: {cpu_tensor}, Device: {cpu_tensor.device}\")\n",
        "\n",
        "# Move the tensor to the GPU (if available)\n",
        "gpu_tensor = cpu_tensor.to(device)\n",
        "print(f\"GPU Tensor: {gpu_tensor}, Device: {gpu_tensor.device}\")\n",
        "\n",
        "# Operations between tensors on different devices are NOT allowed.\n",
        "# Always ensure both tensors are on the same device before performing operations.\n",
        "# Example: If you have data on GPU and a model on CPU, you'll get an error."
      ],
      "metadata": {
        "id": "td0cy38M2ySt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding Tensors is your first big step into the world of `PyTorch`. They are the versatile containers that will hold all your data and model parameters, enabling the powerful computations that drive deep learning models in FinTech and beyond."
      ],
      "metadata": {
        "id": "9LfGMN080Uus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autograd - The Engine Behind Learning\n",
        "If Tensors are the raw materials of deep learning, then Autograd is the sophisticated engine that allows our models to learn. It's the core mechanism that makes PyTorch so powerful and flexible for building neural networks.\n",
        "\n",
        "##The Core of Deep Learning: Calculating Gradients\n",
        "At its heart, training a deep learning model is an optimization problem. We want to find the set of model parameters (weights and biases) that minimizes a \"loss function.\" The loss function tells us how far off our model's predictions are from the true values.\n",
        "\n",
        "To minimize this loss, we use an algorithm called **Gradient Descent** (or one of its many variants). Gradient descent works by iteratively adjusting the model's parameters in the direction that reduces the loss. The \"direction\" is given by the gradient.\n",
        "\n",
        "For neural networks, these gradients need to be calculated with respect to every single parameter in the network, and this process involves applying the chain rule of calculus across potentially millions of operations. Doing this manually would be an impossible task!\n",
        "\n",
        "## `requires_grad=True:` How PyTorch Tracks Operations\n",
        "This is where PyTorch's Autograd system shines. It automates this complex process of calculating gradients. The key to enabling this is the `requires_grad=True` attribute of a Tensor.\n",
        "\n",
        "When you define a tensor with `requires_grad=True`, you're telling PyTorch to keep track of all operations that involve it."
      ],
      "metadata": {
        "id": "lzydqiK032eL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x and y are our \"parameters\" (or inputs we want gradients for)\n",
        "# By setting requires_grad=True, PyTorch will build a computational graph\n",
        "# to track all operations performed with these tensors.\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "print(f\"x requires_grad: {x.requires_grad}\") # Output: True\n",
        "print(f\"y requires_grad: {y.requires_grad}\") # Output: True\n",
        "\n",
        "# z is a result of operations involving x and y\n",
        "# Because x and y require gradients, z will also \"know\" it's part of a gradient computation.\n",
        "z = x**2 + y**3\n",
        "print(f\"z requires_grad: {z.requires_grad}\") # Output: True"
      ],
      "metadata": {
        "id": "A0mOWnx4Tdju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As PyTorch executes operations involving such tensors, it dynamically constructs a computation graph in the background. This graph records how the output (our loss) was derived from the inputs (our parameters).\n",
        "## `.backward():` Computing Gradients\n",
        "Once you have a scalar output tensor (typically your loss value) that was derived from tensors with `requires_grad=True`, you can call the `.backward()` method on it.\n",
        "\n",
        "When `loss.backward()` is executed:\n",
        "\n",
        "PyTorch traverses the computation graph backwards from the loss tensor.\n",
        "It applies the chain rule at each step to compute the gradient of the loss with respect to every tensor that has `requires_grad=True` and was involved in calculating the loss.\n",
        "\n",
        "These computed gradients are then accumulated into the `.grad` attribute of those respective tensors."
      ],
      "metadata": {
        "id": "E9bwuaxu32bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our function: z = x^2 + y^3\n",
        "# x=2, y=3 => z = 2^2 + 3^3 = 4 + 27 = 31\n",
        "\n",
        "# Compute gradients\n",
        "# This populates the .grad attribute for x and y\n",
        "z.backward()"
      ],
      "metadata": {
        "id": "_HgmeO8JUKem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `.grad`: Accessing Gradients\n",
        "After `loss.backward()` has been called, you can access the computed gradients for any tensor that had `requires_grad=True` via its `.grad` attribute.\n",
        "\n",
        "Let's look at our example:\n",
        "\n",
        "$z = x^2 + y^3 $\n",
        "\n",
        "The partial derivative of $z$ with respect to $x$ , $∂z/∂x$ is $2x$. Since $x=2$, $$ \\frac{\\partial z}{\\partial x}=2x=4$$\n",
        "\n",
        "The partial derivative of $z$ with respect to $y$ , $∂z/∂y$ is $3y^2$. Since $y=3$, $$ \\frac{\\partial z}{\\partial y}=3y^2=27$$"
      ],
      "metadata": {
        "id": "TUA6e9Y132Yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print gradients\n",
        "print(f\"Gradient of z with respect to x: {x.grad}\") # Should be 2*x = 4\n",
        "print(f\"Gradient of z with respect to y: {y.grad}\") # Should be 3*y^2 = 27"
      ],
      "metadata": {
        "id": "1fHSYB1-XwSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Zeroing Gradients\n",
        "One crucial aspect of Autograd is that gradients are accumulated. This means if you run `loss.backward()` multiple times without clearing the gradients, the new gradients will be added to the existing ones. For training neural networks, we usually want fresh gradients for each optimization step, so you'll often see `optimizer.zero_grad()` or `tensor.grad.zero_()` before a new backward pass."
      ],
      "metadata": {
        "id": "qBYLfDWF32Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detaching a tensor: Stopping gradient tracking\n",
        "# Sometimes you want to perform an operation but not track gradients for it.\n",
        "# For example, when updating model parameters with the computed gradients.\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "b = a.detach() # b will be a new tensor, but gradient tracking for it is off\n",
        "c = a * 2 # c still tracks gradients because 'a' requires them\n",
        "print(f\"a requires_grad: {a.requires_grad}\") # True\n",
        "print(f\"b requires_grad: {b.requires_grad}\") # False\n",
        "print(f\"c requires_grad: {c.requires_grad}\") # True\n",
        "\n",
        "# When updating parameters, we often use torch.no_grad() context manager\n",
        "# to temporarily disable gradient tracking, ensuring updates are not part of the graph.\n",
        "# Example: W.data -= learning_rate * W.grad (W.data accesses the raw tensor without grad history)\n",
        "# Or, more commonly, within an optimizer's step() method."
      ],
      "metadata": {
        "id": "ZioBKy3lYNLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd is a cornerstone of modern deep learning. It frees us from the tedious and error-prone task of manual gradient computation, allowing us to focus on designing more sophisticated models and understanding the business problems we are trying to solve."
      ],
      "metadata": {
        "id": "ML0en-B532Sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression with PyTorch - Manual Approach"
      ],
      "metadata": {
        "id": "KwpNPavyZOvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "X = torch.randn(100, 1) * 10\n",
        "y = 2 * X + 3 + torch.randn(100, 1) * 3 # y = 2x + 3 + noise\n",
        "\n",
        "# 2. Initialize parameters\n",
        "W = torch.randn(1, 1, requires_grad=True)\n",
        "b = torch.randn(1, 1, requires_grad=True)\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1000\n",
        "\n",
        "print(f\"Initial W: {W.item():.4f}, b: {b.item():.4f}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    y_pred = X @ W + b\n",
        "\n",
        "    # Calculate loss (Mean Squared Error)\n",
        "    loss = ((y_pred - y)**2).mean()\n",
        "\n",
        "    # Backward pass (compute gradients)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters manually using gradients\n",
        "    with torch.no_grad(): # Disable gradient tracking for parameter updates\n",
        "        W -= learning_rate * W.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "        # Zero the gradients for the next iteration\n",
        "        W.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(f\"Final W: {W.item():.4f}, b: {b.item():.4f}\")"
      ],
      "metadata": {
        "id": "f7rZcs_-b5CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch's `nn Module` and `optim` Package\n",
        "Introducing torch.nn: predefined layers (nn.Linear, nn.ReLU, etc.).\n",
        "\n",
        "**nn.Module:** the base class for all neural network modules.\n",
        "Defining a simple neural network using nn.Sequential or by subclassing nn.Module.\n",
        "\n",
        "**torch.optim:** optimizers like SGD, Adam.\n",
        "\n",
        "**torch.nn.MSELoss:** common loss functions.\n",
        "The cleaner training loop."
      ],
      "metadata": {
        "id": "RBnUhY8HfoMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Use the same synthetic data from before\n",
        "X = torch.randn(100, 1) * 10\n",
        "y = 2 * X + 3 + torch.randn(100, 1) * 3\n",
        "\n",
        "# 1. Define the model\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1) # One input feature, one output feature\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = LinearRegression()\n",
        "# Alternatively using nn.Sequential:\n",
        "# model = nn.Sequential(nn.Linear(1, 1))\n",
        "\n",
        "\n",
        "# 2. Define Loss function and Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 1000\n",
        "\n",
        "print(f\"Initial parameters: {list(model.parameters())}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    y_pred = model(X)\n",
        "    loss = criterion(y_pred, y)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad() # Clear previous gradients\n",
        "    loss.backward()       # Compute gradients\n",
        "    optimizer.step()      # Update parameters\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(f\"Final parameters: {list(model.parameters())}\")"
      ],
      "metadata": {
        "id": "Bht6tjUb2vxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the Model\n",
        "\n",
        "It is often useful to call print on the model to see a summary of its structure.The simplest approach is to print the model."
      ],
      "metadata": {
        "id": "8LspIhjGiujF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "wZSPT1KrjsGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other packages available as well to provide richer visualizations of the model. We will explore a few below."
      ],
      "metadata": {
        "id": "CeoIZdBAl-_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##`torchsummary`"
      ],
      "metadata": {
        "id": "Z5zU96UTlzb2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4075e597"
      },
      "source": [
        "!pip install torchsummary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "155382cf"
      },
      "source": [
        "from torchsummary import summary\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# The 'model' variable in the global scope has been overwritten by subsequent cells.\n",
        "# To summarize the LinearRegression model, we need to re-instantiate it.\n",
        "\n",
        "# Define the LinearRegression class (copied from cell Bht6tjUb2vxC for clarity)\n",
        "class LinearRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegression, self).__init__()\n",
        "        self.linear = nn.Linear(1, 1) # One input feature, one output feature\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Create a new instance of the LinearRegression model for summary\n",
        "model_for_summary = LinearRegression()\n",
        "\n",
        "# The 'device' variable is globally defined in cell td0cy38M2ySt.\n",
        "# Ensure the model is explicitly moved to the determined device.\n",
        "print(f\"Model parameters device BEFORE .to(device): {next(model_for_summary.parameters()).device}\")\n",
        "model_for_summary.to(device)\n",
        "print(f\"Model parameters device AFTER .to(device): {next(model_for_summary.parameters()).device}\")\n",
        "\n",
        "# Summarize the LinearRegression model.\n",
        "# input_size=(1,) is correct for this model as it takes a single input feature.\n",
        "print(\"\\nSummary of LinearRegression Model:\")\n",
        "summary(model_for_summary, input_size=(1,), device=str(device))\n",
        "\n",
        "# If you intended to summarize the ImageMLP model, the input_size would be different:\n",
        "# For ImageMLP, the input is 28x28 images, which are flattened to 784.\n",
        "# For example, if 'model' was still the ImageMLP from cZw9oByFANSD:\n",
        "# current_image_mlp_model = ImageMLP() # You'd need to re-instantiate ImageMLP if 'model' was overwritten again\n",
        "# current_image_mlp_model.to(device)\n",
        "# print(\"\\nSummary of ImageMLP Model (example, if that was the intended model):\")\n",
        "# summary(current_image_mlp_model, input_size=(1, 28, 28), device=str(device))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torchviz\n",
        "This library creates a visual graph of the model based on its forward pass. It can show how tensors flow through the network and highlight the operations."
      ],
      "metadata": {
        "id": "-KbkLX9GlZke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "sCoLnUqqlgpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "\n",
        "# Create a dummy input tensor with the correct input shape (batch_size, number_of_features)\n",
        "dummy_input = torch.randn(1, 1)\n",
        "\n",
        "# Make a forward pass to build the computation graph\n",
        "output = model(dummy_input)\n",
        "\n",
        "# Generate the visualization\n",
        "# We can specify the parameters to highlight in the graph\n",
        "dot = make_dot(output, params=dict(list(model.named_parameters())))\n",
        "\n",
        "# Display the visualization (this will typically generate a graph image)\n",
        "dot"
      ],
      "metadata": {
        "id": "c5j4A2m7k6Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##`TensorBoard`\n",
        "TensorBoard is a visualization toolkit for machine learning experiments. PyTorch integrates with it, allowing you to log the model's graph, training metrics, and more.\n",
        "\n",
        "Here are the steps to use TensorBoard with your PyTorch model:\n",
        "\n",
        "**Import necessary libraries:** You'll need torch.utils.tensorboard.SummaryWriter.\n",
        "\n",
        "**Create a SummaryWriter:** This object will be used to write logs to a specified directory.\n",
        "\n",
        "**Add the model graph to TensorBoard:** You can add the model's computational graph by passing a dummy input to the writer's add_graph() method.\n",
        "This requires an instance of your model and a sample input tensor with the correct shape.\n",
        "\n",
        "**Close the writer:** After you're done logging, close the writer.\n",
        "\n",
        "**Launch TensorBoard:** You can launch TensorBoard directly within Google Colab.\n",
        "\n",
        "After running the last cell, a TensorBoard interface should appear below the cell output. Click on the \"Graphs\" tab to see the visualization of your model's structure."
      ],
      "metadata": {
        "id": "o_k9t3m4nfi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Create a SummaryWriter instance. By default, logs will be saved in a 'runs' directory.\n",
        "writer = SummaryWriter('runs/experiment')\n",
        "\n",
        "# Create a dummy input tensor with the correct shape for your MLP model (batch_size, input_features)\n",
        "dummy_input = torch.randn(1, 1)\n",
        "\n",
        "# Add the graph\n",
        "writer.add_graph(model, dummy_input)\n",
        "\n",
        "# Close the writer\n",
        "writer.close()\n",
        "\n",
        "# Launch TensorBoard directly within Google Colab using the %load_ext and %tensorboard magic commands.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/experiment\n"
      ],
      "metadata": {
        "id": "xVjQhCtFpXub",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Simple Multi-Layer Perceptron (MLP) for Classification\n",
        "In the previous section, we built a model for linear regression. Here we will tackle a more complex task: classification. Many problems in FinTech are classification problems, such as:\n",
        "\n",
        "* Will a customer default on a loan? (Yes/No)\n",
        "* Is a transaction fraudulent? (Fraud/Not Fraud)\n",
        "* Will a stock's price go up, down, or stay the same? (Multi-class classification)\n",
        "\n",
        "To do this, we will introduce key concepts that are fundamental to building powerful neural networks:\n",
        "\n",
        "**Non-linearities (Activation Functions):** Rectified Linear Unit (ReLU) allows our model to learn complex, non-linear patterns that a simple linear model cannot.\n",
        "\n",
        "**Output Activations for Probability:** We'll use the Sigmoid activation function in the final layer to produce an output between 0 and 1, which can be interpreted as the probability of belonging to a specific class.\n",
        "\n",
        "**Classification-Specific Loss Function**: We will use Binary Cross-Entropy Loss (nn.BCELoss), which is specifically designed for measuring the error in binary classification tasks."
      ],
      "metadata": {
        "id": "XtdTy76GfHuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Generating a More Realistic and Interesting Dataset\n",
        "The linear regression example used easily separable data. However, real-world financial data is almost never that clean. To build a model that can handle more complex relationships, we need a dataset that is not linearly separable.\n",
        "\n",
        "We will use the `scikit-learn` library to generate a \"moons\" dataset, which is a classic example used to test classification algorithms.\n"
      ],
      "metadata": {
        "id": "Uu4ot3MV-53N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "JOlRe--j-ww2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# 1. Generate a synthetic \"moons\" dataset\n",
        "# We use 500 data points and add some noise to make the task more realistic.\n",
        "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "# This is a critical step in machine learning. We train the model on the training set\n",
        "# and then evaluate its performance on the unseen test set to get an unbiased\n",
        "# measure of how well it generalizes.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Convert NumPy arrays to PyTorch Tensors\n",
        "# PyTorch models require data to be in the form of Tensors.\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "\n",
        "# The labels need to have the same number of dimensions as the model's output,\n",
        "# so we use .view(-1, 1) to add a dimension.\n",
        "y_train = torch.FloatTensor(y_train).view(-1, 1)\n",
        "y_test = torch.FloatTensor(y_test).view(-1, 1)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "Hvi7hCZD_gs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize the data to understand the challenge. The plot will show two intertwined crescent shapes. A simple straight line cannot separate the blue and yellow points, which is why we need a neural network."
      ],
      "metadata": {
        "id": "JNOhzF6U-xhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the data\n",
        "plt.figure(figsize=(8, 5))\n",
        "# Plot training data\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train.squeeze(), cmap='viridis', label='Training Data')\n",
        "# Plot testing data with a different marker\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test.squeeze(), cmap='autumn', marker='x', label='Testing Data')\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.title(\"Synthetic 'Moons' Dataset for Classification\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qoKNZmbCBk_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Defining the MLP Model\n",
        "Our Multi-Layer Perceptron (MLP) will be a simple neural network with one hidden layer. This structure is sufficient to solve this non-linear problem.\n",
        "\n",
        "**Input Layer:** Takes 2 features.\n",
        "\n",
        "**Hidden Layer:** Will have 8 neurons and use the ReLU activation function. ReLU introduces non-linearity, allowing the model to learn the curved boundary between the two moons.\n",
        "\n",
        "**Output Layer:** Will have 1 neuron and use the Sigmoid activation function to output a probability between 0 and 1."
      ],
      "metadata": {
        "id": "kWCSmlLR-xej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        # First linear layer: 2 input features -> 8 hidden units\n",
        "        self.layer1 = nn.Linear(2, 8)\n",
        "        # ReLU activation: introduces non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Second linear layer: 8 hidden units -> 1 output unit\n",
        "        self.layer2 = nn.Linear(8, 1)\n",
        "        # Sigmoid activation: squashes the output to be between 0 and 1\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This defines the forward pass of the network\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of our model\n",
        "model = MLP()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "pVPJN2gfCDgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Training with a Single Batch (The Simple Approach)\n",
        "To start, we will treat our entire training dataset as a single batch. This means we will calculate the loss over all training examples before updating our model's weights in each epoch. While simple, this approach can be computationally expensive and memory-intensive for large datasets.\n",
        "\n",
        "We'll use Binary Cross-Entropy Loss (`nn.BCELoss`), the standard loss function for binary classification, and the `Adam` optimizer, which is a popular and effective alternative to SGD."
      ],
      "metadata": {
        "id": "fI0rNPgy-xbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Loss function and Optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy is suitable for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam is an efficient optimizer\n",
        "\n",
        "num_epochs = 500\n",
        "\n",
        "print(\"Training with a single batch...\")\n",
        "for epoch in range(num_epochs):\n",
        "    # --- Forward pass ---\n",
        "    # Feed the entire training data to the model\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # --- Backward and optimize ---\n",
        "    # 1. Clear the gradients from the previous iteration\n",
        "    optimizer.zero_grad()\n",
        "    # 2. Compute the gradients of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "    # 3. Update the model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # --- Logging and Evaluation ---\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        # To evaluate, we put the model in evaluation mode and use `torch.no_grad()`\n",
        "        # to prevent gradient calculations, which saves memory and computation.\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(X_test)\n",
        "            # Convert outputs probabilities to predicted class (0 or 1)\n",
        "            predicted = (test_outputs > 0.5).float()\n",
        "            # Calculate accuracy\n",
        "            accuracy = (predicted == y_test).float().mean()\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Test Accuracy: {accuracy.item():.4f}\")"
      ],
      "metadata": {
        "id": "SJtydLjnCZ-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Improving Training with Mini-Batches and DataLoader\n",
        "The single-batch approach has limitations. In real-world FinTech applications, datasets can contain millions or even billions of records (e.g., transaction data). Loading all of this into memory is impossible. The solution is mini-batch training. We divide the dataset into small, manageable chunks (mini-batches) and update the model's weights after processing each chunk.\n",
        "\n",
        "Why is mini-batching better?\n",
        "\n",
        "**Memory Efficiency:** It allows us to work with datasets that are far too large to fit into memory.\n",
        "\n",
        "**Faster Convergence:** The model's weights are updated more frequently (once per mini-batch instead of once per epoch), which often leads to faster training.\n",
        "\n",
        "**Improved Generalization:** The \"noise\" introduced by estimating the gradient on a small batch rather than the full dataset can help the model escape poor local minima and find a more robust solution.\n",
        "\n",
        "PyTorch provides two excellent utilities to make this process easy:\n",
        "\n",
        "`TensorDataset`: A class that wraps our features and labels tensors into a single dataset object.\n",
        "\n",
        "`DataLoader`: An iterator that takes a Dataset and provides mini-batches of data, automatically handling shuffling and batching."
      ],
      "metadata": {
        "id": "jHA2lgzo-xYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 1. Create a TensorDataset from our training data\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "\n",
        "# 2. Create a DataLoader\n",
        "batch_size = 32  # A common batch size\n",
        "# shuffle=True is very important for training! It ensures that the data in each\n",
        "# mini-batch is different for every epoch, preventing the model from learning\n",
        "# the order of the data.\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Re-initialize the model and optimizer to start fresh\n",
        "model = MLP()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 500\n",
        "\n",
        "print(\"\\nTraining MLP with DataLoader (Mini-Batches):\")\n",
        "for epoch in range(num_epochs):\n",
        "    # The DataLoader provides an iterator over the mini-batches\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "        # --- Forward pass ---\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        # --- Backward and optimize ---\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # --- Logging and Evaluation (at the end of each epoch) ---\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        with torch.no_grad():\n",
        "            test_outputs = model(X_test)\n",
        "            predicted = (test_outputs > 0.5).float()\n",
        "            accuracy = (predicted == y_test).float().mean()\n",
        "            # The loss here is just for the last batch of the epoch, but accuracy is on the full test set.\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Last Batch Loss: {loss.item():.4f}, Test Accuracy: {accuracy.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "naw5nKzpDhbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A Complete Workflow: MLP on Real-World Data\n",
        "So far, we have worked with synthetically generated data. In most environments, you'll be working with data from external sources, most commonly CSV files. This section will walk you through the complete end-to-end process:\n",
        "\n",
        "* Loading an external dataset using the `pandas` library.\n",
        "* Preprocessing the data, a critical step to prepare it for a neural network.\n",
        "* Building and training an MLP model on this real-world data.\n",
        "\n",
        "We will use a dataset on heart disease prediction which involves predicting a binary outcome based on a set of diverse features.We will first  load the data directly from a URL into a `pandas` DataFrame."
      ],
      "metadata": {
        "id": "2mcd3dQW-xVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset from a URL\n",
        "url = 'http://storage.googleapis.com/download.tensorflow.org/data/heart.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display the first 5 rows to understand its structure\n",
        "print(\"--- First 5 rows of the dataset ---\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for any missing values\n",
        "print(\"\\n--- Missing values per column ---\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "BeByJEIJ7Qbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Preprocessing for the Neural Network\n",
        "Neural networks work best with numerical data and are sensitive to the scale of the input features. For example, a feature like `chol` (cholesterol) has a much larger range of values than `sex`. This can make training unstable. Therefore, we must standardize our data."
      ],
      "metadata": {
        "id": "xi3MFDMt-xSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Separate features (X) from the target label (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Identify non-numeric columns (likely strings or objects)\n",
        "non_numeric_cols = X.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Perform one-hot encoding on all non-numeric columns\n",
        "# pd.get_dummies will convert these columns into numerical representations\n",
        "# drop_first=True avoids multicollinearity by dropping the first category of each feature\n",
        "X = pd.get_dummies(X, columns=non_numeric_cols, drop_first=True)\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Standardize the features\n",
        "# We create a scaler object, fit it ONLY on the training data to learn the mean and standard deviation,\n",
        "# and then use it to transform both the training and testing data.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Convert the processed data into PyTorch Tensors\n",
        "X_train = torch.FloatTensor(X_train_scaled)\n",
        "X_test = torch.FloatTensor(X_test_scaled)\n",
        "y_train = torch.FloatTensor(y_train.values).view(-1, 1)\n",
        "y_test = torch.FloatTensor(y_test.values).view(-1, 1)\n",
        "\n",
        "print(f\"\\nShape of scaled training features: {X_train.shape}\")\n",
        "print(f\"Shape of scaled testing features: {X_test.shape}\")\n",
        "print(f\"Shape of training labels: {y_train.shape}\")\n",
        "print(f\"Shape of testing labels: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "ibd4rOe_7le6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Setting up DataLoaders, Model, and Training Loop\n",
        "This part of the process should now feel familiar. We will use `TensorDataset` and `DataLoader` to create mini-batches, define an MLP with an input size that matches our number of features, and then run the training loop."
      ],
      "metadata": {
        "id": "7MyA2ahh-xPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the model. The input features must match the number of columns in our X data.\n",
        "# The `X_train.shape[1]` programmatically gets this number.\n",
        "class HeartDiseaseMLP(nn.Module):\n",
        "    def __init__(self, input_features):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_features, 20)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(20, 10)\n",
        "        # Adding a second hidden layer for more capacity\n",
        "        self.layer3 = nn.Linear(10, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "model = HeartDiseaseMLP(input_features=X_train.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set the model to training mode\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "        outputs = model(batch_features)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # --- Evaluation at the end of each epoch ---\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        model.eval() # Set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for features, labels in test_loader:\n",
        "                outputs = model(features)\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            accuracy = 100 * correct / total\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "7Se0B8ht80vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSbkZevs-Nfr"
      },
      "source": [
        "## 3. Displaying Predicted vs. Actual Outcomes\n",
        "\n",
        "After training and evaluating the model, it's often helpful to see the model's predictions for individual test examples and compare them to the actual true labels. This can give you a better sense of where the model is making correct predictions and where it might be struggling.\n",
        "\n",
        "The following code cell will iterate through the test dataset using the `test_loader`. For each batch of test data, it will:\n",
        "1.  Pass the features through the trained model to get the predicted probabilities.\n",
        "2.  Convert these probabilities into binary class predictions (0 or 1) by applying a threshold (in this case, 0.5).\n",
        "3.  Collect the actual true labels and the model's predicted labels for all test examples.\n",
        "4.  Finally, it will organize these actual and predicted labels into a pandas DataFrame for clear display."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qflbcl7q-cxn"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Iterate through the test data\n",
        "    for features, labels in test_loader:\n",
        "        # Get model outputs\n",
        "        outputs = model(features)\n",
        "\n",
        "        # Convert probabilities to predicted class (0 or 1)\n",
        "        predicted = (outputs > 0.5).float()\n",
        "\n",
        "        # Append to lists\n",
        "        all_predicted.extend(predicted.squeeze().tolist())\n",
        "        all_labels.extend(labels.squeeze().tolist())\n",
        "\n",
        "# Create a pandas DataFrame to display results\n",
        "results_df = pd.DataFrame({\n",
        "    'Actual Label': all_labels,\n",
        "    'Predicted Label': all_predicted\n",
        "})\n",
        "\n",
        "print(\"\\n--- Predicted vs. Actual Outcomes for Test Data ---\")\n",
        "display(results_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Image Classification with an MLP\n",
        "While MLPs are incredibly versatile, they can even be used for tasks like image classification. In FinTech, this could be applied to problems like signature verification from check images, or document classification from scanned images.\n",
        "We will use the famous MNIST dataset, which consists of 28x28 pixel grayscale images of handwritten digits (0-9). Our goal is to train a model that can correctly identify the digit in each image.\n",
        "\n",
        "To make this work with our MLP, we will employ a simple but powerful trick: we will flatten each 28x28 image into a single, long vector of 784 pixels (28 * 28 = 784). This vector will then be the input to our MLP.\n",
        "\n",
        "##Step 1: Loading and Preparing the MNIST Dataset\n",
        "PyTorch, through its torchvision library, makes it incredibly easy to download and use standard datasets like MNIST."
      ],
      "metadata": {
        "id": "8Qu7lt9c8n1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 1. Define a transformation\n",
        "# `transforms.ToTensor()` converts the image into a PyTorch tensor and scales pixel values to be between 0 and 1.\n",
        "# `transforms.Normalize()` adjusts the tensor values to have a mean of 0.5 and a standard deviation of 0.5.\n",
        "# This helps stabilize training.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# 2. Download and create datasets\n",
        "# PyTorch will automatically download the data to the specified root directory.\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# 3. Create DataLoaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "OUWNy2YM_RGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Visualizing the Data\n",
        "It is always a good idea to look at a few examples from your dataset to make sure everything is as you expect."
      ],
      "metadata": {
        "id": "gQpgeVTn8nyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Function to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # Un-normalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Show the first 4 images from the batch\n",
        "imshow(torchvision.utils.make_grid(images[:4]))\n",
        "# Print the corresponding labels\n",
        "print('Labels:', ' '.join(f'{labels[j]}' for j in range(4)))"
      ],
      "metadata": {
        "id": "MJO5rINS_gl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Defining and Training the Image Classification MLP\n",
        "Our model architecture will be similar to before, but with two key differences:\n",
        "\n",
        "**Input Layer Size:** Will be 784 to accommodate the flattened 28x28 images.\n",
        "**Output Layer Size:** Will be 10, one for each digit (0-9). This is a multi-class classification problem.\n",
        "\n",
        "For multi-class classification, we will use `nn.CrossEntropyLoss`. This loss function is the standard for this type of problem and conveniently includes the final activation function (Softmax) within its calculation for better numerical stability. Therefore, our model will output raw scores (logits), not probabilities."
      ],
      "metadata": {
        "id": "ABghM-Fo8nvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "class ImageMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # The input layer size is 28*28 = 784\n",
        "        self.layer1 = nn.Linear(784, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(128, 64)\n",
        "        # The output layer size is 10 for the 10 digits\n",
        "        self.layer3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The forward pass now includes flattening the image\n",
        "        x = x.view(-1, 28 * 28) # Flatten the image\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        # No final activation function here, as CrossEntropyLoss will apply it\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "model = ImageMLP()\n",
        "# Use CrossEntropyLoss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- Training Loop ---\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"\\nFinished Training!\")\n",
        "\n",
        "# --- Final Evaluation ---\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        # torch.max returns (value, index). We are interested in the index (the predicted class)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the 10,000 test images: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "id": "cZw9oByFANSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even with a simple MLP, you  see a surprisingly high accuracy. This demonstrates the power of neural networks. This also provides a great foundation for Convolutional Neural Networks (CNNs), which are specifically designed for image data and achieve even better performance by not discarding the spatial structure of the pixels."
      ],
      "metadata": {
        "id": "RoMrfVpV8nsP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQUlJ54bCaaP"
      },
      "source": [
        "## Step 4: Visualize some of the predictions\n",
        "This code below visualizes examples of the model's performance on the MNIST test set, showing both correctly and incorrectly predicted images.\n",
        "\n",
        "Here's what the code does:\n",
        "\n",
        "1.  **Set Model to Evaluation Mode**: `model.eval()` puts the model in evaluation mode, which is important for layers that behave differently during training and testing (like dropout, although not used in this specific MLP).\n",
        "2.  **Disable Gradient Calculation**: `with torch.no_grad():` ensures that gradients are not computed during this block. This is not needed for inference and saves memory.\n",
        "3.  **Collect Predictions and Labels**: It iterates through the `test_loader` to get all test images, their actual labels, and the model's predicted labels. These are collected into lists and then concatenated into single tensors.\n",
        "4.  **Identify Correct and Incorrect Predictions**: It compares the `all_predicted` labels with the `all_labels` to find the indices where the predictions were correct and where they were incorrect.\n",
        "5.  **Select Sample Indices**: It randomly selects a specified number of indices (`num_examples_to_show`, set to 5) from both the correct and incorrect prediction sets. This ensures that you see a diverse set of examples each time you run the code.\n",
        "6.  **Display Correct Predictions**: It iterates through the selected correct indices, retrieves the corresponding image, actual label, and predicted label. It then uses `matplotlib` to display the image with the actual and predicted labels in green to indicate a correct prediction. The image is un-normalized (`/ 2 + 0.5`) to display correctly.\n",
        "7.  **Display Incorrect Predictions**: Similarly, it iterates through the selected incorrect indices and displays the images with actual and predicted labels in red to highlight the errors.\n",
        "\n",
        "This visualization helps you quickly assess the types of images your model is handling well and the ones it is struggling with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "420388bc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    all_images = []\n",
        "    all_labels = []\n",
        "    all_predicted = []\n",
        "\n",
        "    # Iterate through the test data\n",
        "    for images, labels in test_loader:\n",
        "        # Get model outputs (logits)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Get predicted class (index of max logit)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Append to lists\n",
        "        all_images.append(images)\n",
        "        all_labels.append(labels)\n",
        "        all_predicted.append(predicted)\n",
        "\n",
        "# Concatenate all batches\n",
        "all_images = torch.cat(all_images)\n",
        "all_labels = torch.cat(all_labels)\n",
        "all_predicted = torch.cat(all_predicted)\n",
        "\n",
        "# Identify correct and incorrect predictions\n",
        "correct_indices = (all_predicted == all_labels).nonzero(as_tuple=True)[0]\n",
        "incorrect_indices = (all_predicted != all_labels).nonzero(as_tuple=True)[0]\n",
        "\n",
        "# Select 5 random correct and 5 random incorrect indices\n",
        "num_examples_to_show = 5\n",
        "if len(correct_indices) > num_examples_to_show:\n",
        "    correct_sample_indices = correct_indices[torch.randperm(len(correct_indices))][:num_examples_to_show]\n",
        "else:\n",
        "    correct_sample_indices = correct_indices\n",
        "\n",
        "if len(incorrect_indices) > num_examples_to_show:\n",
        "    incorrect_sample_indices = incorrect_indices[torch.randperm(len(incorrect_indices))][:num_examples_to_show]\n",
        "else:\n",
        "    incorrect_sample_indices = incorrect_indices\n",
        "\n",
        "# --- Display Correct Predictions ---\n",
        "print(\"--- Examples of Correct Predictions ---\")\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i, idx in enumerate(correct_sample_indices):\n",
        "    plt.subplot(1, num_examples_to_show, i + 1)\n",
        "    # Un-normalize and reshape the image for plotting\n",
        "    img = all_images[idx].squeeze() / 2 + 0.5\n",
        "    plt.imshow(img.numpy(), cmap='gray')\n",
        "    plt.title(f\"Actual: {all_labels[idx].item()}\\nPred: {all_predicted[idx].item()}\", color='green')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Display Incorrect Predictions ---\n",
        "print(\"\\n--- Examples of Incorrect Predictions ---\")\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i, idx in enumerate(incorrect_sample_indices):\n",
        "    plt.subplot(1, num_examples_to_show, i + 1)\n",
        "    # Un-normalize and reshape the image for plotting\n",
        "    img = all_images[idx].squeeze() / 2 + 0.5\n",
        "    plt.imshow(img.numpy(), cmap='gray')\n",
        "    plt.title(f\"Actual: {all_labels[idx].item()}\\nPred: {all_predicted[idx].item()}\", color='red')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Tasks\n",
        "Now that you've learned the fundamentals of PyTorch, it's time to put your skills to the test! These exercises are designed to reinforce the key concepts we covered, from the core mechanics of Autograd to building and training full classification models."
      ],
      "metadata": {
        "id": "K56-IWYnEjbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Required Task 2: Exploring Autograd with a Complex Function\n",
        "This exercise tests your understanding of PyTorch's automatic differentiation engine, Autograd. You will manually define a function with multiple variables, ask PyTorch to compute the gradients, and then verify the results.\n",
        "\n",
        "**Your Goal:** Calculate the partial derivatives of the following function with respect to $w$, $x$, and $b$.\n",
        "\n",
        "\n",
        "Let the function be:\n",
        "$$ z = σ(w.x^2) + {1 \\over b^3}$$\n",
        "\n",
        "\n",
        "where $σ$ is the sigmoid function, which in PyTorch is `torch.sigmoid()`.\n",
        "\n",
        "Given Initial Values:\n",
        "$$w = 2.0$$\n",
        "$$x = 4.0$$\n",
        "$$b = 1.5$$\n",
        "\n",
        "**Your Task:**\n",
        "\n",
        "1. Create three tensors for $w$, $x$, and $b$ with the given initial values. Make sure PyTorch tracks their gradients.\n",
        "\n",
        "2. Write the Python code to compute z using these tensors.\n",
        "\n",
        "3. Call the .backward() method to compute the gradients.\n",
        "\n",
        "4. Print the computed gradients for w, x, and b.\n",
        "\n",
        "Hint: Remember to set `requires_grad=True` when you create the tensors. The gradient of a tensor **t** is stored in **t.grad** after you call `.backward()` on the final output."
      ],
      "metadata": {
        "id": "OQ1f-bbtEoMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Required Task 3: End-to-End Binary Classification - Loan Default Prediction\n",
        "This problem challenges you to apply the full end-to-end workflow on a new dataset. You will predict whether a bank customer is likely to default on a loan, a classic FinTech problem.\n",
        "\n",
        "Your Goal: Build, train, and evaluate an MLP to predict credit risk using the \"German Credit Data\" dataset.\n",
        "\n",
        "**Your Task:**\n",
        "1. Load Data: Load the dataset from the following URL into a pandas DataFrame. The target column is named Creditability.\n",
        "URL: https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
        "\n",
        "Note: This file has no header. You will need to manually assign column names. A list of names is provided in the solution hint.\n",
        "\n",
        "2. Preprocess the Data:\n",
        "* The data is space-separated.\n",
        "* Many columns are categorical (e.g., 'A11', 'A32'). Use one-hot encoding (pd.get_dummies) to convert them into numerical format.\n",
        "* Standardize all numerical features using `StandardScaler`.\n",
        "* Split the data into an 80% training set and a 20% testing set.\n",
        "* Create DataLoaders: Convert your processed data into PyTorch Tensors and create DataLoaders for both the training and test sets.\n",
        "* Build an MLP: Define a new `nn.Module` class for your MLP. It should have at least two hidden layers. You can choose the number of neurons in each layer.\n",
        "* Train the Model: Write a training loop. Use `nn.BCELoss` and the `Adam` optimizer. Train for at least 50 epochs.\n",
        "* Evaluate the Model: After training, calculate and print the final accuracy on the test set.\n",
        "\n",
        "**Hint:**\n",
        "* **Column Names**: You can use this list of column names: [\"Status of existing checking account\", \"Duration in month\", \"Credit history\", \"Purpose\", \"Credit amount\", \"Savings account/bonds\", \"Present employment since\", \"Installment rate in percentage of disposable income\", \"Personal status and sex\", \"Other debtors / guarantors\", \"Present residence since\", \"Property\", \"Age in years\", \"Other installment plans\", \"Housing\", \"Number of existing credits at this bank\", \"Job\", \"Number of people being liable to provide maintenance for\", \"Telephone\", \"foreign worker\", \"Creditability\"].\n",
        "* **Preprocessing**: Remember to apply StandardScaler after you split the data to avoid data leakage from the test set into the training set. Fit the scaler on the training data and use it to transform both train and test data.\n",
        "* **Target Column**: The original target (`Creditability`) has values 1 (Good) and 2 (Bad). You should map this to 0 (Bad) and 1 (Good) to be compatible with `nn.BCELoss`. y = y.replace({1: 1, 2: 0})."
      ],
      "metadata": {
        "id": "vsbt45pUG4n9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Required Task 4: Image Classification with an MLP on Fashion-MNIST\n",
        "This problem tests your ability to adapt the image classification code to a new, slightly more challenging dataset. Fashion-MNIST has the same format as MNIST (28x28 grayscale images, 10 classes) but features clothing items instead of digits, making it a harder task.\n",
        "\n",
        "**Your Goal:** Train an MLP to classify images of clothing from the Fashion-MNIST dataset.\n",
        "\n",
        "**Your Task:**\n",
        "1. **Load Data**: Use `torchvision.datasets` to load the FashionMNIST dataset. The process is almost identical to how you loaded MNIST. Create training and testing DataLoaders.\n",
        "2. **Define a Model**: You can reuse the ImageMLP architecture from the tutorial, as the input (784) and output (10) dimensions are the same.\n",
        "3. **Train the Model**: Write a training loop using `nn.CrossEntropyLoss` and the `Adam` optimizer. Train for 10 epochs.\n",
        "4. **Evaluate**: Calculate and print the final accuracy on the 10,000 test images.\n",
        "5. **Visualize**: Visualize a few correct and incorrect predictions.\n",
        "\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "This is a \"drop-in replacement\" exercise. The vast majority of your MNIST code will work here. You only need to change `torchvision.datasets.MNIST` to `torchvision.datasets.FashionMNIST`. Pay attention to how you define and train the model—it should be a multi-class classification setup."
      ],
      "metadata": {
        "id": "gouIrK-nIOWt"
      }
    }
  ]
}